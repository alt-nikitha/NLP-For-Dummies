{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTKForTermFrequency.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8xTWYBHnfmksJCqXP5Xh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alt-nikitha/NLP-For-Dummies/blob/master/NLTKForTermFrequency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVfLjfCMb7BL",
        "colab_type": "text"
      },
      "source": [
        "This notebook helps us calculate term frequency in a document. This is a very basic and helpful step to do any further analysis in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H313IcXbbWjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install nltk.     Not necessary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saVeqGvTa5HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgL7-lD2boeK",
        "colab_type": "text"
      },
      "source": [
        "Remove stop words like not, and, in, etc because they are common across most documents and don't add much value to the term frequency calculation. NLTK already provides a set of stopwords we can use to check our text against. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqczyUoidAAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ba04c63d-3642-403f-a075-438821f0da44"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTmw68gdRuE",
        "colab_type": "text"
      },
      "source": [
        "Remember to choose english stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLzUvfhlbSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zfum1Q4dWDP",
        "colab_type": "text"
      },
      "source": [
        "Download sample resources given by the nltk library that we can work on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9wGG2lcc4Lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7af6091d-8855-4c80-babf-e7557cc3184e"
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5pyo5c_dg4e",
        "colab_type": "text"
      },
      "source": [
        "Choose one text document since this is a simple tutorial and store the words. Also convert the words to lower case and choose only those words that contain alphabets. Then remove words that are present in the stopwords list we downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-tDUeWFcMhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words=nltk.Text(nltk.corpus.gutenberg.words('bryant-stories.txt'))\n",
        "words=[word.lower() for word in words if(word.isalpha())]\n",
        "words=[word.lower() for word in words if(word not in stop_words)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otqsA6Tsd-1h",
        "colab_type": "text"
      },
      "source": [
        "Let's get the frequency distribution of these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGWkdiw0csTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fDist=FreqDist(words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD0Ic_ajeQDN",
        "colab_type": "text"
      },
      "source": [
        "Now before we go ahead, let us see how many words we have, and how many unique words make up this set. The unique set defines the vocabulary of this text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du6LJenFeJ4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac389244-9af3-4d06-b69b-363657c4a9b5"
      },
      "source": [
        "print(len(words))\n",
        "print(len(set(words)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21718\n",
            "3688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFrFwafNelpF",
        "colab_type": "text"
      },
      "source": [
        "Now let's print the top 10 commonly occurring words in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VINxzBvxeOT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f8f8ce5e-c75f-414d-bb1a-01219454c410"
      },
      "source": [
        "for x,v in fDist.most_common(10):\n",
        "  print(x,v)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "little 597\n",
            "said 453\n",
            "came 191\n",
            "one 183\n",
            "could 158\n",
            "king 141\n",
            "went 122\n",
            "would 112\n",
            "great 110\n",
            "day 107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2jEndZevGp",
        "colab_type": "text"
      },
      "source": [
        "To get a better picture, let us now see what proportion of the entire text these words make up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZJsJDQOekd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "729aa33c-306a-4160-8420-906dd9ef82fc"
      },
      "source": [
        "for x,v in fDist.most_common(10):\n",
        "  print(x,v/len(fDist))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "little 0.1618763557483731\n",
            "said 0.12283080260303687\n",
            "came 0.05178958785249458\n",
            "one 0.04962039045553145\n",
            "could 0.042841648590021694\n",
            "king 0.038232104121475055\n",
            "went 0.03308026030368764\n",
            "would 0.03036876355748373\n",
            "great 0.02982646420824295\n",
            "day 0.02901301518438178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YQ8U2unfBus",
        "colab_type": "text"
      },
      "source": [
        "There is no word that dominates, but we see that around 16 percent of the text is 'little', and 12 percent is 'said'. Whether this SAYS too LITTLE about this text, we will know only after further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfoYymTZ8K_m",
        "colab_type": "text"
      },
      "source": [
        "Another important feature that can help us identify the range of vocabulary used in documents and compare against other texts is the type-token ratio (TTR). This is the ratio of number of unique words to total number of words used in the text. So let us compare the TTR of two texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrRs7_8-9Zjt",
        "colab_type": "text"
      },
      "source": [
        "We need to ensure that the texts we're comparing have the same number of words. This helps us see which text uses a greater vocabulary range with the same word limit.\n",
        "We first perform the same operations we did as before, but choose only the first 15000 words in both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFSwiwQGe4np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_bryant=nltk.Text(nltk.corpus.gutenberg.words('bryant-stories.txt'))\n",
        "words_emma=nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "\n",
        "words_bryant=[word.lower() for word in words_bryant if(word.isalpha())]\n",
        "words_bryant=[word.lower() for word in words_bryant if(word not in stop_words)][:15000]\n",
        "\n",
        "words_emma=[word.lower() for word in words_emma if(word.isalpha())]\n",
        "words_emma=[word.lower() for word in words_emma if(word not in stop_words)][:15000]\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXs92jx795lA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TTR_Bryant= len(set(words_bryant))/len(words_bryant)\n",
        "TTR_Emma=len(set(words_emma))/len(words_emma)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1Oi-UY-CKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54c835cd-d033-4af3-d4b5-9d446abd2a19"
      },
      "source": [
        "print('Bryant: Number of tokens= ',len(words_bryant),'Vocabulary length= ',len(set(words_bryant)))\n",
        "print('Emma: Number of tokens= ',len(words_emma),'Vocabulary length= ',len(set(words_emma)))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bryant: Number of tokens=  15000 Vocabulary length=  2796\n",
            "Emma: Number of tokens=  15000 Vocabulary length=  3274\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}