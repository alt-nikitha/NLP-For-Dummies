{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTKForTermFrequency.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP54YCjDmT5m7fYRTaMl4jm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alt-nikitha/NLP-For-Dummies/blob/master/NLTKForTermFrequency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVfLjfCMb7BL",
        "colab_type": "text"
      },
      "source": [
        "This notebook helps us calculate term frequency in a document. This is a very basic and helpful step to do any further analysis in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H313IcXbbWjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c2afc46-8bdd-4893-d73f-49f2a569c2a1"
      },
      "source": [
        "# !pip install nltk.     Not necessary"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saVeqGvTa5HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "# from nltk.corpus import stopwords"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgL7-lD2boeK",
        "colab_type": "text"
      },
      "source": [
        "Remove stop words like not, and, in, etc because they are common across most documents and don't add much value to the term frequency calculation. NLTK already provides a set of stopwords we can use to check our text against. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqczyUoidAAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "55534b49-9f08-482f-aef0-fc3c2c57a745"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTmw68gdRuE",
        "colab_type": "text"
      },
      "source": [
        "Remember to choose english stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLzUvfhlbSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zfum1Q4dWDP",
        "colab_type": "text"
      },
      "source": [
        "Download sample resources given by the nltk library that we can work on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9wGG2lcc4Lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0013e1d0-7a60-4f6c-82ee-d21c88910541"
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5pyo5c_dg4e",
        "colab_type": "text"
      },
      "source": [
        "Choose one text document since this is a simple tutorial and store the words. Also convert the words to lower case and choose only those words that contain alphabets. Then remove words that are present in the stopwords list we downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-tDUeWFcMhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words=nltk.Text(nltk.corpus.gutenberg.words('bryant-stories.txt'))\n",
        "words=[word.lower() for word in words if(word.isalpha())]\n",
        "words=[word.lower() for word in words if(word not in stop_words)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otqsA6Tsd-1h",
        "colab_type": "text"
      },
      "source": [
        "Let's get the frequency distribution of these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGWkdiw0csTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fDist=FreqDist(words)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD0Ic_ajeQDN",
        "colab_type": "text"
      },
      "source": [
        "Now before we go ahead, let us see how many words we have, and how many unique words make up this set. The unique set defines the vocabulary of this text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du6LJenFeJ4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "44b68ee9-5a34-4f36-c0d0-19495b33392a"
      },
      "source": [
        "print(len(words))\n",
        "print(len(set(words)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21718\n",
            "3688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFrFwafNelpF",
        "colab_type": "text"
      },
      "source": [
        "Now let's print the top 10 commonly occurring words in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VINxzBvxeOT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5a919b14-373b-4056-aaf9-d6c8c7c288f9"
      },
      "source": [
        "for x,v in fDist.most_common(10):\n",
        "  print(x,v)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "little 597\n",
            "said 453\n",
            "came 191\n",
            "one 183\n",
            "could 158\n",
            "king 141\n",
            "went 122\n",
            "would 112\n",
            "great 110\n",
            "day 107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2jEndZevGp",
        "colab_type": "text"
      },
      "source": [
        "To get a better picture, let us now see what proportion of the entire text these words make up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZJsJDQOekd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "69052589-1a3a-44f9-c7e5-498bda93fcda"
      },
      "source": [
        "for x,v in fDist.most_common(10):\n",
        "  print(x,v/len(fDist))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "little 0.1618763557483731\n",
            "said 0.12283080260303687\n",
            "came 0.05178958785249458\n",
            "one 0.04962039045553145\n",
            "could 0.042841648590021694\n",
            "king 0.038232104121475055\n",
            "went 0.03308026030368764\n",
            "would 0.03036876355748373\n",
            "great 0.02982646420824295\n",
            "day 0.02901301518438178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YQ8U2unfBus",
        "colab_type": "text"
      },
      "source": [
        "There is no word that dominates, but we see that around 16 percent of the text is 'little', and 12 percent is 'said'. Whether this SAYS too LITTLE about this text, we will know only after further analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFSwiwQGe4np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}